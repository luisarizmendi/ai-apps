[Unit]
Description=Inference Server GPU Container
After=network-online.target
Wants=network-online.target

[Container]
Image=nvcr.io/nvidia/tritonserver:25.03-py3-igpu
ContainerName=triton-inference-gpu

# NET
Network=host

# Volume and mount configuration
Volume=/models:/mnt/models:z
Tmpfs=/dev/shm

# Security and device configuration
SecurityLabelDisable=true
AddDevice=nvidia.com/gpu=all

# Environment variables
Environment=PORT=8000
Environment=PORT=8001

# Command to execute
Exec=/bin/sh -c 'exec tritonserver --model-repository=/mnt/models --allow-http=true'

[Service]
ExecStartPre=/bin/sleep 10
Restart=always
TimeoutStartSec=900
RestartSec=30

[Install]
WantedBy=default.target
