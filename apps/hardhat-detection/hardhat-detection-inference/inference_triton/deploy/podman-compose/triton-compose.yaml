
version: '3.8'

services:
  model-downloader:
    image: quay.io/luisarizmendi/model-downloader:latest
    container_name: model-downloader-gpu
    volumes:
      - model-storage-gpu:/mnt/models:z
    environment:
      - IMAGE_NAME=${IMAGE_NAME:-quay.io/luisarizmendi/modelcar-hardhat:v1}
    restart: "no"
    devices:
      - /dev/fuse:/dev/fuse
    networks:
      - hardhat-network


  inference-container:
    image: nvcr.io/nvidia/tritonserver:25.03-py3
    container_name: inference-container-gpu
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    ports:
      - "8000:8000"  # REST API
      - "8001:8001"  # gRPC API
    volumes:
      - model-storage-gpu:/mnt/models:z
    tmpfs:
      - /dev/shm
    environment:
      - PORT=8000
    # GPU support - uncomment the following lines if you have GPU support
    # devices:
    #   - nvidia.com/gpu=all
    # security_opt:
    #   - label=disable
    command: >
      /bin/sh -c 'exec tritonserver 
      "--model-repository=/mnt/models" 
      "--allow-http=true" 
      "--allow-sagemaker=false"'
    restart: unless-stopped
    networks:
      - hardhat-network

volumes:
  model-storage-gpu:
    driver: local

networks:
  hardhat-network:
    driver: bridge
